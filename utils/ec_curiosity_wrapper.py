"""
Dan Iulian Muntean 2020
Parts copied from https://github.com/google-research/episodic-curiosity/blob/master/episodic_curiosity/curiosity_env_wrapper.py
Wrapper around a Gym environment to add curiosity reward."""

from __future__ import absolute_import
from __future__ import division

from __future__ import print_function

import utils.episodic_memory as ep_mem
from torch_rl.utils import ParallelEnv
import numpy as np
import torch
from copy import deepcopy


def get_observation_embedding_fn(r_model):

    def _observation_embedding_fn(x):
        r_model.eval()
        with torch.no_grad():
            embeddings =  r_model.forward(x)
        r_model.train()

        return embeddings

    return _observation_embedding_fn


def get_observation_compare_fn(r_model):

    def _observation_compare_fn(x1, x2):
        r_model.eval()
        with torch.no_grad():
            rez = r_model.forward_similarity(x1, x2)
        r_model.train()

        return rez

    return _observation_compare_fn


class CuriosityEnvWrapper(ParallelEnv):
    """Environment wrapper that adds additional curiosity reward."""
    def __init__(self,
                 envs,
                 cfg,
                 device,
                 observation_preprocess_fn,
                 r_model):

        super(CuriosityEnvWrapper, self).__init__(envs)

        self._r_model = r_model
        self._device = device
        self._observation_preprocess_fn = observation_preprocess_fn
        self._observation_embedding_fn = get_observation_embedding_fn(r_model)
        self._observation_compare_fn = get_observation_compare_fn(r_model)

        self._scale_task_reward = getattr(cfg, "scale_task_reward", 1.0)
        self._scale_surrogate_reward = getattr(cfg, "scale_surrogate_reward", 0.0)
        self._bonus_reward_additive_term = getattr(cfg, "bonus_reward_additive_term", 0)
        self._exploration_reward_min_step = getattr(cfg, "exploration_reward_min_step", 0)
        self._similarity_threshold = getattr(cfg, "similarity_threshold", 0.5)
        self._exploration_reward = getattr(cfg, "exploration_reward", "none")

        # Create an episodic memory for each env
        replacement_strategy = getattr(cfg, "replacement_strategy", "fifo")
        memory_capacity = getattr(cfg, "memory_capacity", 200)
        embedding_size = getattr(cfg, "embedding_size", [512])

        self._episodic_memories = [ep_mem.EpisodicMemory(embedding_size,
                                                         self._observation_compare_fn,
                                                         self._device,
                                                         replacement_strategy,
                                                         memory_capacity) for _ in range(self.no_envs + 1)]

        # Total number of steps so far per environment.
        self._step_count = 0

        # Observers are notified each time a new time step is generated by the
        # environment.
        # Observers implement a function "on_new_observation".
        self._observers = []

    def _compute_curiosity_reward(self, observations, infos, dones):
        """Compute intrinsic curiosity reward.
        The reward is set to 0 when the episode is finished
        """

        frames = self._observation_preprocess_fn(observations, device=self._device)
        embedded_observations = self._observation_embedding_fn(frames)

        similarity_to_memory = [
            ep_mem.similarity_to_memory(embedded_observations[k],
                                        self._episodic_memories[k])
            for k in range(self.no_envs + 1)
        ]

        # Updates the episodic memory of every environment.
        for k in range(self.no_envs + 1):
            # If we've reached the end of the episode, resets the memory
            # and always adds the first state of the new episode to the memory.
            if dones[k]:
                self._episodic_memories[k].reset()
                self._episodic_memories[k].add(embedded_observations[k].cpu(), infos[k])
                continue

            # Only add the new state to the episodic memory if it is dissimilar
            # enough.
            if similarity_to_memory[k] < self._similarity_threshold:
                self._episodic_memories[k].add(embedded_observations[k].cpu(), infos[k])

        # Augment the reward with the exploration reward.
        bonus_rewards = [
            0.0 if d else 0.5 - s + self._bonus_reward_additive_term
            for (s, d) in zip(similarity_to_memory, dones)
        ]
        bonus_rewards = np.array(bonus_rewards)

        return bonus_rewards

    def reset(self):

        observations = super(CuriosityEnvWrapper, self).reset()
        for k in range(self.no_envs + 1):
            self._episodic_memories[k].reset()

        return observations

    def step(self, actions):

        obs, rewards, dones, infos = super(CuriosityEnvWrapper, self).step(actions)

        for observer in self._observers:
            observer.on_new_observation(obs, rewards, dones, infos)

        self._step_count += 1

        # Exploration bonus.
        reward_for_input = None
        if self._exploration_reward == 'episodic_curiosity':
            bonus_rewards = self._compute_curiosity_reward(obs, infos, dones)
            reward_for_input = bonus_rewards

            for i in range(self.no_envs + 1):
                infos[i]["ir"] = bonus_rewards[i]

        elif self._exploration_reward == 'none':
            bonus_rewards = np.zeros(self.no_envs + 1)
            reward_for_input = np.zeros(self.no_envs + 1)

        else:
            raise ValueError('Unknown exploration reward: {}'.format(self._exploration_reward))

        # Combined rewards.
        scale_surrogate_reward = self._scale_surrogate_reward
        if self._step_count < self._exploration_reward_min_step:
            # This can be used for online training during the first N steps,
            # the R network is totally random and the surrogate reward has no
            # meaning.
            scale_surrogate_reward = 0.0

        postprocessed_rewards = (self._scale_task_reward * np.array(rewards) +
                                 scale_surrogate_reward * bonus_rewards)

        return obs, postprocessed_rewards, dones, infos

    def add_observer(self, observer):
        self._observers.append(observer)

    def get_episodic_memeories(self):
        return self._episodic_memories

    def get_episodic_memory(self, k):
        """Returns the episodic memory for the k-th environment."""
        return self._episodic_memories[k]

    def render(self):
        raise NotImplementedError

    def __len__(self):
        return self.no_envs + 1

    def __getitem__(self, item):
        return self.envs[item]

