"""
Dan Iulian Muntean 2020
Parts copied from https://github.com/google-research/episodic-curiosity/blob/master/episodic_curiosity/curiosity_env_wrapper.py
Wrapper around a Gym environment to add curiosity reward."""

from __future__ import absolute_import
from __future__ import division

from __future__ import print_function

import utils.episodic_memory as ep_mem
from torch_rl.utils import ParallelEnv
import numpy as np


class CuriosityEnvWrapper(ParallelEnv):
    """Environment wrapper that adds additional curiosity reward."""
    def __init__(self,
                 envs,
                 cfg,
                 observation_shape,
                 observation_preprocess_fn,
                 observation_embedding_fn,
                 observation_compare_fn):

        super(CuriosityEnvWrapper, self).__init__(envs)

        self._observation_preprocess_fn = observation_preprocess_fn
        self._observation_embedding_fn = observation_embedding_fn
        self._observation_compare_fn = observation_compare_fn

        self._scale_task_reward = getattr(cfg, "scale_task_reward", 1.0)
        self._scale_surrogate_reward = getattr(cfg, "scale_surrogate_reward", 0.0)
        self._bonus_reward_additive_term = getattr(cfg, "bonus_reward_additive_term", 0)
        self._exploration_reward_min_step = getattr(cfg, "exploration_reward_min_step", 0)
        self._similarity_threshold = getattr(cfg, "similarity_threshold", 0.5)
        self._exploration_reward = getattr(cfg, "exploration_reward", "none")

        # Create an episodic memory for each env
        replacement_strategy = getattr(cfg, "replacement_strategy", "fifo")
        memory_capacity = getattr(cfg, "memory_capacity", 200)

        self._episodic_memories = [ep_mem.EpisodicMemory(observation_shape,
                                                  self._observation_compare_fn,
                                                  replacement_strategy,
                                                  memory_capacity) for _ in range(self.no_envs + 1)]

        # Total number of steps so far per environment.
        self._step_count = 0

        # Observers are notified each time a new time step is generated by the
        # environment.
        # Observers implement a function "on_new_observation".
        self._observers = []

    def _compute_curiosity_reward(self, observations, dones, infos):
        """Compute intrinsic curiosity reward.
        The reward is set to 0 when the episode is finished
        """

        frames = self._observation_preprocess_fn(observations)
        embedded_observations = self._observation_embedding_fn(frames)

        similarity_to_memory = [
            ep_mem.similarity_to_memory(embedded_observations[k],
                                        self._episodic_memories[k])
            for k in range(self.no_envs + 1)
        ]

        # Updates the episodic memory of every environment.
        for k in range(self.no_envs + 1):
            # If we've reached the end of the episode, resets the memory
            # and always adds the first state of the new episode to the memory.
            if dones[k]:
                self._episodic_memories[k].reset()
                self._episodic_memories[k].add(embedded_observations[k], infos[k])
                continue

            # Only add the new state to the episodic memory if it is dissimilar
            # enough.
            if similarity_to_memory[k] < self._similarity_threshold:
                self._episodic_memories[k].add(embedded_observations[k], infos[k])

        # Augment the reward with the exploration reward.
        bonus_rewards = [
            0.0 if d else 0.5 - s + self._bonus_reward_additive_term
            for (s, d) in zip(similarity_to_memory, dones)
        ]
        bonus_rewards = np.array(bonus_rewards)

        return bonus_rewards

    def reset(self):

        observations = super(CuriosityEnvWrapper, self).reset()
        for k in range(self.no_envs + 1):
            self._episodic_memories[k].reset()

        return observations

    def step(self, actions):

        obs, rewards, dones, infos = super(CuriosityEnvWrapper, self).step(actions)

        for observer in self._observers:
            # TODO return also intrinsic rewards
            observer.on_new_observation(obs, rewards, dones, infos)

        self._step_count += 1

        # Exploration bonus.
        reward_for_input = None
        if self._exploration_reward == 'episodic_curiosity':
            bonus_rewards = self._compute_curiosity_reward(obs, infos, dones)
            reward_for_input = bonus_rewards

        elif self._exploration_reward == 'none':
            bonus_rewards = np.zeros(self.no_envs + 1)
            reward_for_input = np.zeros(self.no_envs + 1)

        else:
            raise ValueError('Unknown exploration reward: {}'.format(self._exploration_reward))

        # Combined rewards.
        #scale_surrogate_reward = self._scale_surrogate_reward
        #if self._step_count < self._exploration_reward_min_step:
            # This can be used for online training during the first N steps,
            # the R network is totally random and the surrogate reward has no
            # meaning.
        #    scale_surrogate_reward = 0.0

        #postprocessed_rewards = (self._scale_task_reward * rewards +
        #                           scale_surrogate_reward * bonus_rewards)

        return obs, (rewards, bonus_rewards), dones, infos

    def add_observer(self, observer):
        self._observers.append(observer)

    def get_episodic_memeories(self):
      return self._episodic_memories

    def get_episodic_memory(self, k):
      """Returns the episodic memory for the k-th environment."""
      return self._episodic_memories[k]

    def render(self):
        raise NotImplementedError