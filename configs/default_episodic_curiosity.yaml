main:
  env: MiniGrid-DoorKey-8x8-v0 # required=True, help="name of the environment to train on (REQUIRED)")
  seed: 0 # type=int, default=1, help="random seed (default: 1)")
  procs: 16 # type=int, default=16, help="number of processes (default: 16)") # number of envs
  actual_procs: 3 # type=int, default=16, help="number of processes (default: 16)")
  frames: 3.e+6 # type=int, default=10**7, help="number of frames of training (default: 10e7)")
  eval_interval: 10
  log_interval: 1 # type=int, default=1, help="number of updates between two logs (default: 1)")
  save_interval: 0 # type=int, default=0, help="number of updates between two saves (default: 0, 0 means no saving)")
  tb: false # action="store_true", default=False, help="log into Tensorboard")
  text: &text false # action="store_true", default=False, help="add a GRU to the model to handle text  input")

  max_eprews: 0.93 # Cap training when mean episode rewards reaches this value
  save_best: true
  save_all: true

env_cfg:
  max_episode_steps: 200 # environment steps
  max_image_value: 15.
  normalize: true
  wrapper: ""
  no_eval_envs: 0
  no_actions: 6 # 7 is for done

agent:
  name: PPOEpisodicCuriosity
  frames_per_proc: 128 # type=int, default=None, help="number of frames per process before update (default: 5 for A2C and 128 for PPO)")
  discount: 0.99 # type=float, default=0.99, help="discount factor (default: 0.99)")
  optimizer: Adam
  optimizer_args:
    lr: 0.001
    eps: 1.e-8
  gae_lambda: 0.95 # type=float, default=0.95, help="lambda coefficient in GAE formula (default: 0.95, 1 means no gae)")
  entropy_coef: 0.01 # type=float, default=0.01, help="entropy term coefficient (default: 0.01)")
  value_loss_coef: 0.5 # type=float, default=0.5, help="value loss term coefficient (default: 0.5)")
  max_grad_norm: 0.5 # type=float, default=0.5, help="maximum norm of gradient (default: 0.5)")
  clip_eps: 0.2 # type=float, default=0.2, help="clipping epsilon for PPO (default: 0.2)")
  epochs: 4 # type=int, default=4, help="number of epochs for PPO (default: 4)")
  batch_size: 256 # type=int, default=256, help="batch size for PPO (default: 256)")
  recurrence: &recurrence 1 # type=int, default=1, help="number of timesteps gradient is
  min_mem: &min_mem 0  # If min number of recurrence to use mem
  # backpropagated (default: 1) If > 1, a LSTM is added to the model to have memory")
  exp_used_pred: 0.25 # #type=float, default=0.25, help="proportion of experience used for training predictor
  running_norm_obs: false # normalize the observations using a running mean

  # Parameters for the EPISODIC CURIOSITY MODULE

  # Parameters for computing the intrinsic reward and combining rewards
  scale_task_reward: 1.0                    # extrinsic reward coefficient
  scale_surrogate_reward: 0.005               # intrinsic reward coefficient
  bonus_reward_additive_term: 0             # constant value (an epsilon) to add to intrinsic reward
  exploration_reward_min_step: 10000        # threshold for starting to compute intrinsic rewards
  similarity_threshold: 0.9                # threshold for adding a new embedding to the episodic memory
  replacement_stragegy: "fifo"              # episodic memory replacement strategy
  memory_capacity: 200                      # size of episodic memory
  exploration_reward: "episodic_curiosity"  # select if intrinsic reward is computed
  embedding_size: [512]                      # size of embedded observation

  # Parameters for training the similarity network
  r_net_batch_size: 64                      # batch size when training the curiosity module RNetwork
  r_net_num_epochs: 10                       # number of epochs to train the curiosity module RNetwork
  r_net_observation_history_size: 6000     # number of observations used for training once
  r_net_training_interval: 3000            # number of steps between two consecutive training periods
  r_net_training_data_type: "v123"            # tells the process of creating the dataset from observations
  r_net_max_action_distance: 5              # given two obs: o_i, o_j, they are reachable for each other if |o_i - o_j| <= value
  r_net_avg_num_examples_per_env_step: 1    # number of datapoints created from each env step
  r_net_negative_sample_multiplier: 5       # used to create a gap between positive and negative example
  r_net_optimizer: Adam                     # optimizer used when training the RNetwork
  r_net_optimizer_args:                     # optimizer parameters
      lr: 1.e-4
      betas: [0.9, 0.999]
      eps: 1.e-8
  r_net_initial_training_budget: 9000     # number of steps taken in the environment for training only RNetwork

model:
  name: EpisodicCuriosityModel
  recurrence: *recurrence
  min_mem: *min_mem
  text: *text
  memory_type: GRU
  use_memory: true
  k_sizes: [2, 2, 2]
  s_sizes: [1, 1, 1]
  # Parameters for the R-Network training used to compute reachability between observations
  r_net_k_sizes: [2, 2, 2]            # kernel size for embedding extractor
  r_net_s_sizes: [1, 1, 1]            # stride size for embedding extractor
  r_net_comp_sizes: [512, 512, 512]   # comparator layers nr units
  r_net_embedding_size: 512           # embedding extractor features size
  similarity_computation: "complex"   # type of comparator used for similarity
  # simple - uses only dot product and a sigmoid, while complex - uses a feed forward network

extra_logs: [[return_int_per_episode, rIntR, 2f]]
